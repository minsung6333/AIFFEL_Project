{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minsung6333/AIFFEL_Project/blob/main/18_%EB%B2%88%EC%97%AD%EA%B0%80%EB%8A%94_%EB%8C%80%ED%99%94%EC%97%90%EB%8F%84_%EB%8A%A5%ED%95%98%EB%8B%A4_%5B%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18-1. Project: 멋진 챗봇 만들기\n",
        "## 라이브러리 버전을 확인해 봅니다\n",
        "사용할 라이브러리 버전을 둘러봅시다."
      ],
      "metadata": {
        "id": "1vEiTNRwoXgL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e9Ro0JhoUR0"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import tensorflow\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "print(numpy.__version__)\n",
        "print(pandas.__version__)\n",
        "print(tensorflow.__version__)\n",
        "print(nltk.__version__)\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지난 노드에서 챗봇과 번역기는 같은 집안이라고 했던 말을 기억하시나요?\n",
        "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해 봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해 보세요!\n",
        "\n",
        "## Step 1. 데이터 다운로드\n",
        "준비하기 단계에서 심볼릭 링크를 생성했다면 아래 파일이 ChatbotData .csv라는 이름으로 저장되어 있을거예요. csv 파일을 읽는 데에는 pandas 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하세요!\n",
        "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n",
        "## Step 2. 데이터 정제\n",
        "아래 조건을 만족하는 preprocess_sentence() 함수를 구현하세요.\n",
        "\n",
        "1. 영문자의 경우, 모두 소문자로 변환합니다.\n",
        "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
        "문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!\n",
        "## Step 3. 데이터 토큰화\n",
        "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
        "\n",
        "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!\n",
        "\n",
        "소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
        "데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
        "토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
        "토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
        "중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
        "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다.\n",
        "## Step 4. Augmentation\n",
        "우리에게 주어진 데이터는 1만 개가량으로 적은 편에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해 봐야겠죠? Lexical Substitution을 실제로 적용해 보도록 하겠습니다.\n",
        "\n",
        "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻으세요!\n",
        "\n",
        "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
        "다운로드한 모델을 활용해 데이터를 Augmentation 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
        "\n",
        "Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n",
        "## Step 5. 데이터 벡터화\n",
        "타겟 데이터인 ans_corpus 에 <start> 토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ans_corpus 는 list 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
      ],
      "metadata": {
        "id": "o10sO8QJofmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
        "\n",
        "print([\"<start>\"] + sample_data + [\"<end>\"])"
      ],
      "metadata": {
        "id": "EfiJLSQ5o_VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 위 소스를 참고하여 타겟 데이터 전체에 <start> 토큰과 <end> 토큰을 추가해 주세요!\n",
        "\n",
        "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
        "\n",
        "3.특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train 과 dec_train 을 얻으세요!\n",
        "## Step 6. 훈련하기\n",
        "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다."
      ],
      "metadata": {
        "id": "mpSOszPDpBPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예문\n",
        "1. 지루하다, 놀러가고 싶어.\n",
        "2. 오늘 일찍 일어났더니 피곤하다.\n",
        "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
        "4. 집에 있는다는 소리야.\n",
        "\n",
        "---\n",
        "\n",
        "# 제출\n",
        "\n",
        "Translations\n",
        "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
        "> 2. 맛난 거 드세요 . <end>\n",
        "> 3. 떨리 겠 죠 . <end>\n",
        "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
        "\n",
        "Hyperparameters\n",
        "> n_layers: 1\n",
        "> d_model: 368\n",
        "> n_heads: 8\n",
        "> d_ff: 1024\n",
        "> dropout: 0.2\n",
        "\n",
        "Training Parameters\n",
        "> Warmup Steps: 1000\n",
        "> Batch Size: 64\n",
        "> Epoch At: 10"
      ],
      "metadata": {
        "id": "vkrL32sSpHMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7. 성능 측정하기\n",
        "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수도 적용해 보세요"
      ],
      "metadata": {
        "id": "X6TJbbv0pIxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 평가문항                                        | 상세기준                                                                                       |\n",
        "|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
        "| 1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?              | 챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.           |\n",
        "| 2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?                          | 과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다. |\n",
        "| 3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?         | 주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.                                             |\n"
      ],
      "metadata": {
        "id": "vAQfBeHgpMpl"
      }
    }
  ]
}